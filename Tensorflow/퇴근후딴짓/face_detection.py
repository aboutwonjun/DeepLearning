# -*- coding: utf-8 -*-
"""Face_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xARROaMqLUnWcEU9bKBhOjDpXmA92tSF
"""



from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# 라이브러리 불러오기
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from tensorflow.keras.layers import Dense, Flatten, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

# %matplotlib inline

path = "/content/drive/MyDrive/challenges-in-representation-learning-facial-expression-recognition-challenge/"
#Dataset
train = pd.read_csv(path + "train.csv")
test = pd.read_csv(path + "test.csv")
data = pd.read_csv(path+'icml_face_data.csv')

train.shape, test.shape, data.shape
#data 만 활용

data.head()

#데이터 종류
data[' Usage'].value_counts()

data['emotion'].value_counts()
# 0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'

data['emotion'].value_counts().plot(kind='barh', figsize=(8, 4))

#데이터 불균형이 존재

data[' pixels'][0]

len(data[' pixels'][0]) 

#데이터 사이즈는 48*48인데.. 왜케 많냐 / 데이터 타입이 오브젝트로 되어있기 떄문. 타입 바꾸기

temp = np.fromstring(data[' pixels'][0], dtype=int, sep=' ') #형변환 string _> int 
print(len(temp))

#데이터를 전처리해야한다. 

def preprocessing(Data):
  image = np.zeros(shape=(len(data), 48,48)) #0만 들어간 값으로 변형
  label = np.array(list(map(int,data['emotion']))) 

  for i,row in enumerate(data.index):
    df = np.fromstring(data[' pixels'][row], dtype=int,sep= ' ')
    df = np.reshape(df,(48,48))
    image[i] = df
  return image,label

# Train, Validation, Test 데이터 구분
x_train, y_train = preprocessing(data[data[' Usage']=='Training'])
x_val, y_val = preprocessing(data[data[' Usage']=='PrivateTest'])
x_test, y_test = preprocessing(data[data[' Usage']=='PublicTest'])

x_train.shape, x_val.shape, x_test.shape

plt.imshow(x_train[0], cmap='gray')
print("lable:",y_train[0])

plt.imshow(x_train[100], cmap='gray')
print("lable:",y_train[100])

# 모델1
model = Sequential([
                    Flatten(input_shape=(48,48)),
                    Dense(128, activation='relu'),
                    Dense(7, activation='softmax')
])
# 모델 컴파일
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 훈련(학습)
history = model.fit(x_train, y_train,  validation_data=(x_val, y_val), epochs=5, batch_size=64)

# 모델2
model = Sequential([
                    Flatten(input_shape=(48,48)),
                    BatchNormalization(),
                    Dense(128, activation='relu'),
                    BatchNormalization(),
                    Dense(7, activation='softmax')
])
# 모델 컴파일
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 훈련(학습)
history = model.fit(x_train, y_train,  validation_data=(x_val, y_val), epochs=5, batch_size=64)

loss, acc = model.evaluate(x_test, y_test)
print('test정확도:', acc)

from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import EarlyStopping

# 데이터 전처리(shape 변경)
print(x_train.shape, x_val.shape, x_test.shape)
x_train = x_train.reshape((x_train.shape[0], 48, 48, 1))
x_val = x_val.reshape((x_val.shape[0], 48, 48, 1))
x_test = x_test.reshape((x_test.shape[0], 48, 48, 1))
print(x_train.shape, x_val.shape, x_test.shape)

x_train = x_train.astype('float32')/x_train.max()
x_val = x_val.astype('float32')/x_val.max()
x_test = x_test.astype('float32')/x_test.max()

# 모델3 CNN 적용
model = Sequential([
                    Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),
                    MaxPooling2D((2, 2)),
                    Conv2D(64, (3, 3), activation='relu'),
                    MaxPooling2D((2, 2)),
                    Flatten(),
                    Dense(128, activation='relu'),
                    Dense(7, activation='softmax')
])
# 모델 컴파일
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_accuracy', patience=3) 

# 훈련(학습)
history = model.fit(x_train, y_train,  validation_data=(x_val, y_val), epochs=30, batch_size=64, callbacks=[early_stopping])

